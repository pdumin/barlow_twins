{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "from torchvision import models\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ya_transforms = T.Compose(\n",
    "    [\n",
    "        T.ToTensor(),\n",
    "        T.RandomChoice(\n",
    "            [\n",
    "                T.ColorJitter(),\n",
    "                T.RandomHorizontalFlip(),\n",
    "                T.GaussianBlur(3),\n",
    "                T.RandomSolarize(.6)\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "Yb_transforms = T.Compose(\n",
    "    [\n",
    "        T.ToTensor(),\n",
    "        T.RandomChoice(\n",
    "            [\n",
    "                T.ColorJitter(),\n",
    "                T.RandomHorizontalFlip(),\n",
    "                T.GaussianBlur(3),\n",
    "                T.RandomSolarize(.6)\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_dataset = MNIST(\n",
    "    'data',\n",
    "    train = 'True',\n",
    "    download = True,\n",
    "    transform=Ya_transforms\n",
    ")\n",
    "\n",
    "\n",
    "B_dataset = MNIST(\n",
    "    'data',\n",
    "    train = 'True',\n",
    "    download = True,\n",
    "    transform=Yb_transforms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=256\n",
    "\n",
    "A_loader = DataLoader(A_dataset, batch_size=BATCH_SIZE)\n",
    "B_loader = DataLoader(B_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]]]),\n",
       " tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1,\n",
       "         1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5,\n",
       "         9, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0, 4, 5, 6, 1, 0, 0, 1, 7,\n",
       "         1, 6, 3, 0, 2, 1, 1, 7, 9, 0, 2, 6, 7, 8, 3, 9, 0, 4, 6, 7, 4, 6, 8, 0,\n",
       "         7, 8, 3, 1, 5, 7, 1, 7, 1, 1, 6, 3, 0, 2, 9, 3, 1, 1, 0, 4, 9, 2, 0, 0,\n",
       "         2, 0, 2, 7, 1, 8, 6, 4, 1, 6, 3, 4, 5, 9, 1, 3, 3, 8, 5, 4, 7, 7, 4, 2,\n",
       "         8, 5, 8, 6, 7, 3, 4, 6, 1, 9, 9, 6, 0, 3, 7, 2, 8, 2, 9, 4, 4, 6, 4, 9,\n",
       "         7, 0, 9, 2, 9, 5, 1, 5, 9, 1, 2, 3, 2, 3, 5, 9, 1, 7, 6, 2, 8, 2, 2, 5,\n",
       "         0, 7, 4, 9, 7, 8, 3, 2, 1, 1, 8, 3, 6, 1, 0, 3, 1, 0, 0, 1, 7, 2, 7, 3,\n",
       "         0, 4, 6, 5, 2, 6, 4, 7, 1, 8, 9, 9, 3, 0, 7, 1, 0, 2, 0, 3, 5, 4, 6, 5,\n",
       "         8, 6, 3, 7, 5, 8, 0, 9, 1, 0, 3, 1, 2, 2, 3, 3])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(A_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 4, 1),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 16, 6, 2),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16)\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.LazyLinear(16)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paveldumin/miniforge3/envs/pt/lib/python3.8/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 16])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_data = torch.randn(256, 1, 28, 28)\n",
    "\n",
    "Encoder()(fake_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Projector(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(in_features=16, out_features=32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.proj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 16])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Projector()(torch.randn(256, 16)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F_theta(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder()\n",
    "        self.projector = Projector()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.projector(self.encoder(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paveldumin/miniforge3/envs/pt/lib/python3.8/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "model = F_theta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "F_theta                                  --\n",
       "├─Encoder: 1-1                           --\n",
       "│    └─Sequential: 2-1                   --\n",
       "│    │    └─Conv2d: 3-1                  544\n",
       "│    │    └─Dropout: 3-2                 --\n",
       "│    │    └─ReLU: 3-3                    --\n",
       "│    │    └─BatchNorm2d: 3-4             64\n",
       "│    └─Sequential: 2-2                   --\n",
       "│    │    └─Conv2d: 3-5                  18,448\n",
       "│    │    └─Dropout: 3-6                 --\n",
       "│    │    └─ReLU: 3-7                    --\n",
       "│    │    └─BatchNorm2d: 3-8             32\n",
       "│    └─Sequential: 2-3                   --\n",
       "│    │    └─LazyLinear: 3-9              --\n",
       "├─Projector: 1-2                         --\n",
       "│    └─Sequential: 2-4                   --\n",
       "│    │    └─Linear: 3-10                 544\n",
       "│    │    └─BatchNorm1d: 3-11            64\n",
       "│    │    └─ReLU: 3-12                   --\n",
       "│    │    └─Linear: 3-13                 528\n",
       "=================================================================\n",
       "Total params: 20,224\n",
       "Trainable params: 20,224\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    return (x - x.mean(0)) / x.std(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch n: 0, loss: 3.3138827334059044\n",
      "Epoch n: 1, loss: 1.2140538943574783\n",
      "Epoch n: 2, loss: 0.9484281950808586\n",
      "Epoch n: 3, loss: 0.8098967663785245\n",
      "Epoch n: 4, loss: 0.7613271729743227\n",
      "Epoch n: 5, loss: 0.7083654177949784\n",
      "Epoch n: 6, loss: 0.6771670341491699\n",
      "Epoch n: 7, loss: 0.650630200796939\n",
      "Epoch n: 8, loss: 0.6249631072612519\n",
      "Epoch n: 9, loss: 0.6046989103581043\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(10):\n",
    "    batch_losses = []\n",
    "    for x_a, x_b in zip(A_loader, B_loader):\n",
    "        out_a = model(x_a[0])\n",
    "        out_b = model(x_b[0])\n",
    "\n",
    "        out_a_normed = normalize(out_a)\n",
    "        out_b_normed = normalize(out_b)\n",
    "\n",
    "        c = torch.matmul(out_a_normed.T, out_b_normed) / BATCH_SIZE\n",
    "\n",
    "        c_diff = (c - torch.eye(c.shape[0])).pow(2)\n",
    "\n",
    "        c_diff.flatten()[:-1].view(c_diff.size(0) - 1, c_diff.size(0)+1)[:, 1:].mul_(.5)\n",
    "\n",
    "        loss = c_diff.sum()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.item())\n",
    "    losses.append(np.mean(batch_losses))\n",
    "\n",
    "    print(f'Epoch n: {epoch}, loss: {losses[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:05:16) \n[Clang 12.0.1 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a7df815f77bf8edb0a6a656480e7ee3c360a4b494ba862a12bd6032824eda046"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
